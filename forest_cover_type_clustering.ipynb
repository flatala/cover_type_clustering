{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Installing the dependencies and importing required stuff"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%pip install -r requrements.txt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from pprint import pprint\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import make_scorer, homogeneity_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "from scipy.stats import mode\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Exploration & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset = pd.read_csv('covtype.csv')\n",
    "\n",
    "X = dataset.drop(columns=['Cover_Type'])\n",
    "y = dataset['Cover_Type']\n",
    "\n",
    "dataset.head(3)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Helper functions for data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_label_distribution(column):\n",
    "    if not isinstance(column, pd.Series):\n",
    "        raise ValueError(\"Input must be a pandas Series.\")\n",
    "\n",
    "    label_counts = column.value_counts().sort_index()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = label_counts.plot(kind='bar')\n",
    "    plt.title(f\"Distribution of Labels in Column '{column.name}'\", fontsize=14)\n",
    "    plt.xlabel(\"Labels\", fontsize=12)\n",
    "    plt.ylabel(\"Count\", fontsize=12)\n",
    "    plt.xticks(rotation=45, fontsize=10)\n",
    "    \n",
    "    max_count = label_counts.max()\n",
    "    ax.set_ylim(0, max_count * 1.1)\n",
    "    for i, count in enumerate(label_counts):\n",
    "        ax.text(i, count + (0.02 * max_count), str(count), ha='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def one_hot_correlation_plot(dataframe, target_column, method='spearman'):\n",
    "    if target_column not in dataframe.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in the dataframe.\")\n",
    "\n",
    "    target_one_hot = pd.get_dummies(dataframe[target_column], prefix=target_column)\n",
    "    numeric_features = dataframe.drop(columns=[target_column]).select_dtypes(include=[np.number])\n",
    "    \n",
    "    if numeric_features.empty:\n",
    "        raise ValueError(\"No numeric features available for correlation analysis.\")\n",
    "    \n",
    "    correlations = {}\n",
    "    for col in target_one_hot.columns:\n",
    "        corr = numeric_features.corrwith(target_one_hot[col], method=method)\n",
    "        correlations[col] = corr.sort_values(ascending=False)\n",
    "    \n",
    "    for category, corr_series in correlations.items():\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.barplot(x=corr_series.index, y=corr_series.values, palette=\"coolwarm\", edgecolor=\"black\")\n",
    "        plt.title(f\"{method.capitalize()} Correlation with Target Category: '{category}'\", fontsize=12)\n",
    "        plt.ylabel('Correlation Coefficient', fontsize=12)\n",
    "        plt.xlabel(\"\")  \n",
    "        plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def average_absolute_correlation_plot(dataframe, target_column, method='spearman'):\n",
    "    if target_column not in dataframe.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in the dataframe.\")\n",
    "\n",
    "    target_one_hot = pd.get_dummies(dataframe[target_column], prefix=target_column)\n",
    "    numeric_features = dataframe.drop(columns=[target_column]).select_dtypes(include=[np.number])\n",
    "    \n",
    "    if numeric_features.empty:\n",
    "        raise ValueError(\"No numeric features available for correlation analysis.\")\n",
    "\n",
    "    correlation_matrix = []\n",
    "    for col in target_one_hot.columns:\n",
    "        corr = numeric_features.corrwith(target_one_hot[col], method=method).abs()\n",
    "        correlation_matrix.append(corr)\n",
    "\n",
    "    correlation_df = pd.DataFrame(correlation_matrix, index=target_one_hot.columns).T\n",
    "    average_absolute_correlation = correlation_df.mean(axis=1)\n",
    "    average_absolute_correlation = average_absolute_correlation.sort_values(ascending=False)\n",
    "\n",
    "    # Plot the average absolute correlations\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x=average_absolute_correlation.index, y=average_absolute_correlation.values, palette=\"coolwarm\", edgecolor=\"black\")\n",
    "    plt.title(f\"Average Absolute {method.capitalize()} Correlation with Target Classes\", fontsize=12)\n",
    "    plt.ylabel('Average Absolute Correlation Coefficient', fontsize=12)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_low_correlation_features(dataframe, target_column, threshold=0.1, method='spearman'):\n",
    "    if target_column not in dataframe.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in the dataframe.\")\n",
    "\n",
    "    target_one_hot = pd.get_dummies(dataframe[target_column], prefix=target_column)\n",
    "    numeric_features = dataframe.drop(columns=[target_column]).select_dtypes(include=[np.number])\n",
    "\n",
    "    if numeric_features.empty:\n",
    "        raise ValueError(\"No numeric features available for correlation analysis.\")\n",
    "\n",
    "    correlation_matrix = []\n",
    "    for col in target_one_hot.columns:\n",
    "        corr = numeric_features.corrwith(target_one_hot[col], method=method).abs()\n",
    "        correlation_matrix.append(corr)\n",
    "\n",
    "    correlation_df = pd.DataFrame(correlation_matrix, index=target_one_hot.columns).T\n",
    "    low_correlation_features = correlation_df[(correlation_df < threshold).all(axis=1)].index.tolist()\n",
    "\n",
    "    if low_correlation_features:\n",
    "        print(f\"Features with absolute correlation below {threshold} for all target categories:\")\n",
    "        for feature in low_correlation_features:\n",
    "            print(f\"- {feature}\")\n",
    "        return low_correlation_features\n",
    "    else:\n",
    "        print(f\"No features found with absolute correlation below {threshold} for all target categories.\")\n",
    "        return []\n",
    "    \n",
    "def analyze_features(df):\n",
    "    \"\"\"\n",
    "    Analyzes features in the DataFrame and prints their types and value ranges.\n",
    "    \"\"\"\n",
    "    feature_summary = {}\n",
    "\n",
    "    for column in df.columns:\n",
    "        unique_values = df[column].nunique()\n",
    "        total_values = len(df[column])\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            if unique_values == 2:\n",
    "                feature_type = \"Binary\"\n",
    "            else:\n",
    "                feature_type = \"Numerical\"\n",
    "            value_range = (df[column].min(), df[column].max())\n",
    "        else:\n",
    "            feature_type = \"Categorical\"\n",
    "            value_range = df[column].unique()\n",
    "\n",
    "        feature_summary[column] = {\n",
    "            \"Type\": feature_type,\n",
    "            \"Value Range\": value_range,\n",
    "        }\n",
    "\n",
    "    for feature, details in feature_summary.items():\n",
    "        print(f\"Feature: {feature}\")\n",
    "        print(f\"  Type: {details['Type']}\")\n",
    "        print(f\"  Value Range: {details['Value Range']}\")\n",
    "        print(\"-\")\n",
    "\n",
    "def plot_max_correlation_for_features(dataframe, target_column, method='spearman'):\n",
    "    if target_column not in dataframe.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in the dataframe.\")\n",
    "    \n",
    "    target_one_hot = pd.get_dummies(dataframe[target_column], prefix=target_column)\n",
    "    numeric_features = dataframe.drop(columns=[target_column]).select_dtypes(include=[np.number])\n",
    "\n",
    "    if numeric_features.empty:\n",
    "        raise ValueError(\"No numeric features available for correlation analysis.\")\n",
    "    \n",
    "    correlation_matrix = []\n",
    "    for col in target_one_hot.columns:\n",
    "        corr = numeric_features.corrwith(target_one_hot[col], method=method)\n",
    "        correlation_matrix.append(corr)\n",
    "\n",
    "    correlation_df = pd.DataFrame(correlation_matrix, index=target_one_hot.columns).T\n",
    "    \n",
    "    max_correlation_values = correlation_df.max(axis=1)\n",
    "    \n",
    "    sorted_correlation = max_correlation_values.sort_values(ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(sorted_correlation.index, sorted_correlation.values, color='skyblue')\n",
    "    plt.title(\"Maximum Correlation for Each Feature with Labels\")\n",
    "    plt.ylabel(\"Correlation\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.xticks(rotation=45, ha='right')  \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Performing Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at feature types and value ranges"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "analyze_features(dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "numerical_features = dataset.iloc[:, :10]\n",
    "description = numerical_features.describe()\n",
    "desc_filtered = description.drop(index=['25%', '50%', '75%', 'count'])\n",
    "desc_transposed = desc_filtered.T\n",
    "\n",
    "print(desc_transposed)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "plt.subplots(figsize=(21, 14))\n",
    "\n",
    "color = sns.color_palette('pastel')\n",
    "sns.boxplot(data = numerical_features, orient='h', palette=color)\n",
    "\n",
    "plt.title('Spread of data in Numerical Features', size = 20)\n",
    "plt.xlabel('Values', size = 17)\n",
    "plt.ylabel('Features', size = 17)\n",
    "plt.xticks(size = 17)\n",
    "plt.yticks(size = 15)\n",
    "\n",
    "sns.despine()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_pca = dataset.drop(columns=['Cover_Type'])\n",
    "y_pca = dataset['Cover_Type']\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_embedded = pca.fit_transform(X_pca)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "num_points = 10000\n",
    "sc = ax.scatter(X_embedded[:num_points, 0], X_embedded[:num_points, 1], X_embedded[:num_points, 2], c=y_pca[:num_points], cmap='viridis', alpha=0.6)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel(\"PCA Component 1\")\n",
    "ax.set_ylabel(\"PCA Component 2\")\n",
    "ax.set_zlabel(\"PCA Component 3\")\n",
    "ax.set_title(\"3D PCA Visualization of Cover Type Dataset\")\n",
    "\n",
    "# Colorbar\n",
    "plt.colorbar(sc, label=\"Classes\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE Visualization (We also did 3D)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "X_tsne = dataset.drop(columns=['Cover_Type'])\n",
    "y_tsne = dataset['Cover_Type']\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "X_embedded = tsne.fit_transform(X_tsne)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_embedded[:, 0], y=X_embedded[:, 1], hue=y_tsne, palette=\"deep\", alpha=0.7)\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.title(\"t-SNE Visualization of COver Type Dataset\")\n",
    "plt.legend(title=\"Classes\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot label distribution to look for imbalances"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_label_distribution(y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at feature corelations with target"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "average_absolute_correlation_plot(dataset, 'Cover_Type', 'spearman')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_max_correlation_for_features(dataset, target_column='Cover_Type')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify all features with max absolute correlation to any of the one hot encoded targets below 0.2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "low_correlation_features = print_low_correlation_features(dataset, 'Cover_Type', threshold=0.2, method='spearman')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we reduce the dataset to be 2747 sample per class, which limits the size to reduce the runtime of algorithms, and also rebalances the dataset. Then we drop the low colleration features. We set X and y to the redistributed version, and from this point on we rely on that in the code."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset_dropped = dataset.drop(columns=low_correlation_features) # drop redundant features\n",
    "max_samples_per_label = 2747 # reduce the dataset size to contain at most 2747 samples per class, this also rebalances the dataset\n",
    "\n",
    "dataset_reduced = (\n",
    "    dataset_dropped.groupby('Cover_Type', group_keys=False)\n",
    "      .apply(lambda x: x.sample(n=min(len(x), max_samples_per_label), random_state=42))\n",
    ")\n",
    "\n",
    "dataset_reduced = dataset_reduced.reset_index(drop=True)\n",
    "\n",
    "X = dataset_reduced.drop(columns=['Cover_Type'])\n",
    "y = dataset_reduced['Cover_Type']\n",
    "\n",
    "plot_label_distribution(y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we explore the pca reduction. We want to see how much variance can we capture, etc"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pca = PCA(n_components=None)\n",
    "pca.fit(X)\n",
    "\n",
    "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "for i, cum_var in enumerate(cumulative_variance, start=1):\n",
    "    if i > 20:\n",
    "        break\n",
    "    print(f\"Num Principal Components {i}: Explained Variance = {cum_var:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title(f'PCA Explained Variance ')\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Classifier Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset to train and test."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "def classifiers_nested_grid_search(estimator, param_grid, experiment_description, X, y):\n",
    "    outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)  # Outer CV\n",
    "    inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)  # Inner CV\n",
    "\n",
    "    best_params_list = []\n",
    "    nested_scores = []\n",
    "\n",
    "    classification_reports = []\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(outer_cv.split(X, y)):\n",
    "        print(f\"Starting outer fold {fold_idx + 1}...\")\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, scoring='accuracy', cv=inner_cv, n_jobs=-1, verbose=3)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        best_params_list.append(str(grid_search.best_params_))  \n",
    "\n",
    "        best_estimator = grid_search.best_estimator_\n",
    "        \n",
    "        y_pred = best_estimator.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        nested_scores.append(accuracy)\n",
    "\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        classification_reports.append(class_report)\n",
    "\n",
    "        print(f\"Accuracy for outer fold {fold_idx + 1}: {accuracy:.4f}\")\n",
    "        print(f\"Classification Report for fold {fold_idx + 1}:\\n\", class_report)\n",
    "\n",
    "    most_common_params = Counter(best_params_list).most_common(1)[0][0]\n",
    "    best_params = eval(most_common_params)  \n",
    "\n",
    "    return {\n",
    "        'model': estimator.__class__.__name__,\n",
    "        'best_params': best_params,\n",
    "        'nested_cv_score': sum(nested_scores) / len(nested_scores),  # Average accuracy from nested CV\n",
    "        'classification_reports': classification_reports,\n",
    "        'description': experiment_description,\n",
    "    }\n",
    "\n",
    "svc_clf = SVC()\n",
    "knn_clf = KNeighborsClassifier()\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "svc_param_grid = {\n",
    "    'C': [0.1, 1, 10, 100], \n",
    "    'kernel': ['poly', 'rbf', 'linear'],\n",
    "    'gamma': ['scale', 0.001, 0.1],\n",
    "}\n",
    "\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['distance', 'uniform'],\n",
    "    'metric': ['manhattan', 'euclidean', 'minkowski'],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "\n",
    "dt_param_grid = {\n",
    "    'criterion': ['log_loss', 'entropy', 'log_loss'],\n",
    "    'splitter': ['random', 'best'],\n",
    "    'max_depth': [None, 3, 5, 7, 9, 11],\n",
    "    'min_samples_split': [2, 3, 4, 5, 6, 7],\n",
    "    'min_samples_leaf': [1, 2, 5, 7]\n",
    "}\n",
    "\n",
    "description = \"SVM Grid Search Nested CV Example\"\n",
    "nested_grid_search_results_svc = classifiers_nested_grid_search(svc_clf, svc_param_grid, description, X_train, y_train)\n",
    "\n",
    "description = \"KNN Grid Search Nested CV Example\"\n",
    "nested_grid_search_results_knn = classifiers_nested_grid_search(knn_clf, knn_param_grid, description, X_train, y_train)\n",
    "\n",
    "description = \"Decision Tree Grid Search Nested CV Example\"\n",
    "nested_grid_search_results_dt = classifiers_nested_grid_search(dt_clf, dt_param_grid, description, X_train, y_train)\n",
    "\n",
    "pprint(nested_grid_search_results_svc)\n",
    "pprint(nested_grid_search_results_knn)\n",
    "pprint(nested_grid_search_results_dt)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "svc_clf = SVC(C=100, kernel='rbf', gamma='scale')\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5, weights='distance', metric='manhattan', n_jobs=-1)\n",
    "dt_clf = DecisionTreeClassifier(criterion='log_loss', splitter='random', max_depth=None, min_samples_split=2, min_samples_leaf=1)\n",
    "\n",
    "\n",
    "svc_clf.fit(X_train, y_train)\n",
    "knn_clf.fit(X_train, y_train)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "svc_y_pred = svc_clf.predict(X_test)\n",
    "knn_y_pred = knn_clf.predict(X_test)\n",
    "dt_y_pred = dt_clf.predict(X_test)\n",
    "\n",
    "svc_accuracy = accuracy_score(y_test, svc_y_pred)\n",
    "knn_accuracy = accuracy_score(y_test, knn_y_pred)\n",
    "dt_accuracy = accuracy_score(y_test, dt_y_pred)\n",
    "\n",
    "svc_class_report = classification_report(y_test, svc_y_pred)\n",
    "knn_class_report = classification_report(y_test, knn_y_pred)\n",
    "dt_class_report = classification_report(y_test, dt_y_pred)\n",
    "\n",
    "print(\"SVM Model Performance:\")\n",
    "print(f\"Accuracy: {svc_accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", svc_class_report)\n",
    "\n",
    "print(\"KNN Model Performance:\")\n",
    "print(f\"Accuracy: {knn_accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", knn_class_report)\n",
    "\n",
    "print(\"Decision Tree Model Performance:\")\n",
    "print(f\"Accuracy: {dt_accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", dt_class_report)\n",
    "\n",
    "results = {\n",
    "    'SVM': {\n",
    "        'accuracy': svc_accuracy,\n",
    "        'classification_report': svc_class_report\n",
    "    },\n",
    "    'KNN': {\n",
    "        'accuracy': knn_accuracy,\n",
    "        'classification_report': knn_class_report\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'accuracy': dt_accuracy,\n",
    "        'classification_report': dt_class_report\n",
    "    }\n",
    "}\n",
    "\n",
    "pprint(results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the optimisation loop over the param grid, pca component counts, differnt scalers, different models, etc."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import homogeneity_score, accuracy_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import json\n",
    "from scipy.stats import mode\n",
    "\n",
    "def scale_selected_columns(df, columns_to_scale, scaler):\n",
    "    if scaler is None:\n",
    "        return df\n",
    "    scaled_data = scaler.fit_transform(df[columns_to_scale])\n",
    "    df_scaled = df.copy()\n",
    "    df_scaled[columns_to_scale] = scaled_data\n",
    "    return df_scaled\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import mode\n",
    "import numpy as np\n",
    "\n",
    "def clustering_accuracy(y_true, y_pred):\n",
    "    unique_clusters = np.unique(y_pred)\n",
    "    cluster_to_label = {}\n",
    "\n",
    "    for cluster in unique_clusters:\n",
    "        mask = y_pred == cluster\n",
    "        majority_label = mode(y_true[mask], keepdims=True).mode\n",
    "\n",
    "        # ensure majority_label is properly indexed\n",
    "        if isinstance(majority_label, np.ndarray) and majority_label.size > 0:\n",
    "            majority_label = majority_label[0]\n",
    "        \n",
    "        cluster_to_label[cluster] = majority_label\n",
    "\n",
    "    y_pred_mapped = np.array([cluster_to_label[cluster] for cluster in y_pred])\n",
    "    return accuracy_score(y_true, y_pred_mapped)\n",
    "\n",
    "\n",
    "def clustering_grid_search(estimator, param_grid, scaling_type, pca_count, X, y):\n",
    "    outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "    clustering_accuracy_scorer = make_scorer(clustering_accuracy)\n",
    "    # accuracy_scorer = make_scorer(accuracy_score)\n",
    "    # homogeneity_scorer = make_scorer(homogeneity_score)\n",
    "\n",
    "    best_params_list = []\n",
    "    nested_scores = []\n",
    "\n",
    "    for train_idx, test_idx in outer_cv.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, scoring=clustering_accuracy_scorer, cv=inner_cv, n_jobs=-1, verbose=1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        best_params_list.append(str(grid_search.best_params_))\n",
    "\n",
    "        best_estimator = grid_search.best_estimator_\n",
    "        nested_score = clustering_accuracy(y_test, best_estimator.fit(X_train, y_train).predict(X_test))\n",
    "        nested_scores.append(nested_score)\n",
    "\n",
    "    most_common_params = Counter(best_params_list).most_common(1)[0][0]\n",
    "    best_params = eval(most_common_params)\n",
    "\n",
    "    pca_info = 'no_pca'\n",
    "    if pca_count is not None:\n",
    "        pca_info = pca_count\n",
    "\n",
    "    return {\n",
    "        'model': estimator.__class__.__name__,\n",
    "        'best_params': best_params,\n",
    "        'nested_cv_score': sum(nested_scores) / len(nested_scores),\n",
    "        'scaler': scaling_type,\n",
    "        'pca_count': pca_info,\n",
    "        'dim': len(X.columns)\n",
    "    }\n",
    "\n",
    "def complete_optimization_clustering(models, param_grids, X, y, pca_counts, scaling_types):\n",
    "    clustering_grid_search_results = []\n",
    "    best_nested_cv_score = 0\n",
    "    best_run = None\n",
    "    \n",
    "    for pca_count in pca_counts:    \n",
    "        if pca_count is not None:\n",
    "            print('\\n#############################################################')\n",
    "            print(f'                  Running for {pca_count} components')\n",
    "            print('#############################################################\\n')\n",
    "            pca = PCA(n_components=pca_count)\n",
    "            X_processed_1 = pd.DataFrame(pca.fit_transform(X), columns=[f'pca{i+1}' for i in range(pca_count)], index=X.index)\n",
    "        else:\n",
    "            print('\\n#############################################################')\n",
    "            print(f'                     Running without PCA')\n",
    "            print('#############################################################\\n')\n",
    "            X_processed_1 = X\n",
    "        \n",
    "        for scaling_type in scaling_types:\n",
    "            numerical_columns = X_processed_1.select_dtypes(include=['number']).columns.tolist()\n",
    "            columns_to_scale = [col for col in numerical_columns if X_processed_1[col].nunique() > 2]\n",
    "            scaler = MinMaxScaler() if scaling_type == 'min_max_scaler' else StandardScaler() if scaling_type == 'standard_scaler' else None\n",
    "            X_processed_2 = scale_selected_columns(X_processed_1, columns_to_scale, scaler)\n",
    "            \n",
    "            print('start ------------------------------------------------------- \\n')\n",
    "            print('Running models for the following scaler and pca config:')\n",
    "            description = f'scaler: {scaling_type}, num_principal_components: {pca_count}'\n",
    "            print(description)\n",
    "\n",
    "            for model, params in zip(models, param_grids):\n",
    "                result = clustering_grid_search(model, params, scaling_type, pca_count, X_processed_2, y)\n",
    "\n",
    "                if result['nested_cv_score'] > best_nested_cv_score:\n",
    "                    best_nested_cv_score = result['nested_cv_score']\n",
    "                    best_run = result\n",
    "                \n",
    "                clustering_grid_search_results.append(result)\n",
    "                pprint(result)\n",
    "\n",
    "            print('\\nend ---------------------------------------------------------\\n\\n')\n",
    "    \n",
    "    return clustering_grid_search_results, best_run"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 KNN-CLustering Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "km_clustering = KMeans()\n",
    "\n",
    "km_param_grid = {\n",
    "    'n_clusters': [50],\n",
    "    'init': ['k-means++', 'random'],\n",
    "    'max_iter': [300, 500, 1000],         \n",
    "    'tol': [1e-4, 1e-3, 1e-2],      \n",
    "    'algorithm': ['lloyd', 'elkan'],\n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "clustering_grid_search_results, best_run = complete_optimization_clustering([km_clustering], [km_param_grid], X_train, y_train, [None, 3, 5, 7, 10], ['min_max_scaler', 'standard_scaler', None])\n",
    "\n",
    "print('\\n\\nBEST RUN:\\n')\n",
    "pprint(best_run)\n",
    "\n",
    "with open(\"k_means_results\", \"w\") as json_file:\n",
    "    json.dump(clustering_grid_search_results, json_file, indent=4) "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot impact of cluster number on K-Means"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.15, random_state=42)\n",
    "\n",
    "\n",
    "n_clusters = np.arange(5, 1001, 5)\n",
    "accuracies = []\n",
    "\n",
    "for clusters in n_clusters:\n",
    "    km_clustering = KMeans(n_clusters=clusters, init='random', algorithm='lloyd', max_iter=300, tol=0.001, random_state=42)\n",
    "    km_clustering.fit(X_train)\n",
    "    \n",
    "    y_pred = km_clustering.predict(X_test)\n",
    "    \n",
    "    # Assuming clustering_accuracy is defined elsewhere\n",
    "    accuracy = clustering_accuracy(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_clusters, accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Clustering Accuracy')\n",
    "plt.title('Impact of Number of Clusters on K-Means Clustering Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Affinity Propagation Grid Search (Don't run this locally, your machine will just crash)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "ap_param_grid = {\n",
    "    'damping': [0.5, 0.9],  # Controls cluster stability\n",
    "    'preference': [-50, -10, 0],  # Influences the number of clusters\n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "ap_clustering = AffinityPropagation()\n",
    "clustering_grid_search_results, best_run = complete_optimization_clustering([ap_clustering], [ap_param_grid], X_train, y_train, [None, 3, 5, 7, 10], ['min_max_scaler', 'standard_scaler', None])\n",
    "\n",
    "with open(\"affinity_propagation_results\", \"w\") as json_file:\n",
    "    json.dump(clustering_grid_search_results, json_file, indent=4) "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3. MeanShift Clustering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "ms_param_grid = {\n",
    "    'bandwidth': ['auto', 1, 3, 5, 7, 15]\n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "ms_clustering = MeanShift()\n",
    "\n",
    "clustering_grid_search_results, best_run = complete_optimization_clustering([ms_clustering], [ms_param_grid], X_train, y_train, [None, 3, 5, 7, 10], ['min_max_scaler', 'standard_scaler', None])\n",
    "\n",
    "with open(\"mean_shift_results.json\", \"w\") as json_file:\n",
    "    json.dump(clustering_grid_search_results, json_file, indent=4) "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4. Birch Clustering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.cluster import Birch\n",
    "\n",
    "birch_param_grid = {\n",
    "    'n_clusters': [50],\n",
    "    'threshold': [0.5, 1.0, 1.5],\n",
    "    'branching_factor': [20, 50]\n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "birch_clustering = Birch()\n",
    "\n",
    "clustering_grid_search_results, best_run = complete_optimization_clustering([birch_clustering], [birch_param_grid], X_train, y_train, [None, 3, 5, 7, 10], ['min_max_scaler', 'standard_scaler', None])\n",
    "\n",
    "with open(\"birch_results_0_20_v2.json\", \"w\") as json_file:\n",
    "    json.dump(clustering_grid_search_results, json_file, indent=4) "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Some plotting based on the results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_scaling_impact_on_homogeneity_bar(clustering_grid_search_results):\n",
    "    df = pd.DataFrame(clustering_grid_search_results)\n",
    "    \n",
    "    required_columns = {'scaler', 'nested_cv_score', 'model'}\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        raise ValueError(\"Results must contain 'scaler', 'nested_cv_score', and 'model' keys.\")\n",
    "    \n",
    "    df['scaler'] = df['scaler'].apply(lambda x: 'None' if x is None else x)\n",
    "    \n",
    "    summary = (\n",
    "        df.groupby(['model', 'scaler'])['nested_cv_score']\n",
    "        .agg(['mean', 'min', 'max'])\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    summary['scaler_order'] = summary['scaler'].apply(lambda x: 0 if x == 'None' else 1)\n",
    "    summary = summary.sort_values(by=['model', 'scaler_order', 'scaler']).drop(columns='scaler_order')\n",
    "    \n",
    "    models = summary['model'].unique()\n",
    "    scalers = summary['scaler'].unique()\n",
    "    \n",
    "    x = np.arange(len(scalers))\n",
    "    total_models = len(models)\n",
    "    bar_width = 0.8 / total_models \n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    for idx, model_name in enumerate(models):\n",
    "        model_data = summary[summary['model'] == model_name]\n",
    "        means = model_data['mean']\n",
    "        mins = model_data['min']\n",
    "        maxs = model_data['max']\n",
    "\n",
    "        lower_errors = means - mins\n",
    "        upper_errors = maxs - means\n",
    "        error = [lower_errors, upper_errors]\n",
    "        \n",
    "        offset = (idx - total_models / 2) * bar_width + bar_width / 2\n",
    "        positions = x + offset\n",
    "        \n",
    "        plt.bar(\n",
    "            positions,\n",
    "            means,\n",
    "            bar_width,\n",
    "            label=model_name,\n",
    "            yerr=error,\n",
    "            capsize=5,\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    plt.xlabel(\"Scaler\")\n",
    "    plt.ylabel(\"Custom Accuracy\")\n",
    "    plt.title(\"Impact of Scaling on Clustering Accuracy\")\n",
    "    plt.xticks(x, scalers, rotation=45)\n",
    "    plt.legend(title='Model')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_pca_impact_on_accuracy_best_result(clustering_grid_search_results):\n",
    "    df = pd.DataFrame(clustering_grid_search_results)\n",
    "    \n",
    "    required_columns = {'pca_count', 'nested_cv_score', 'model'}\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        raise ValueError(\n",
    "            \"Results must contain 'pca_count', 'nested_cv_score', and 'model' keys.\"\n",
    "        )\n",
    "    \n",
    "    def to_numeric_pca_count(x):\n",
    "        if x == 'no_pca':\n",
    "            return 0\n",
    "        return pd.to_numeric(x, errors='coerce')  # convert strings like \"10\" to int; invalid become NaN\n",
    "    \n",
    "    df['pca_count'] = df['pca_count'].apply(to_numeric_pca_count)\n",
    "    \n",
    "    best_df = (\n",
    "        df.groupby(['model', 'pca_count'])['nested_cv_score']\n",
    "        .max()\n",
    "        .reset_index()\n",
    "        .rename(columns={'nested_cv_score': 'best_score'})\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for model_name, sub_df in best_df.groupby('model'):\n",
    "        sub_df = sub_df.sort_values('pca_count')\n",
    "        plt.plot(\n",
    "            sub_df['pca_count'],\n",
    "            sub_df['best_score'],\n",
    "            marker='o',\n",
    "            label=f\"{model_name}\"\n",
    "        )\n",
    "    \n",
    "    unique_pca_counts = sorted(best_df['pca_count'].dropna().unique())\n",
    "    x_labels = [\"no_pca\" if x == 0 else str(int(x)) for x in unique_pca_counts]\n",
    "    plt.xticks(unique_pca_counts, x_labels)\n",
    "\n",
    "    plt.xlabel(\"Number of PCA Components\")\n",
    "    plt.ylabel(\"Accuracy Score (Best Parameters)\")\n",
    "    plt.title(\"Best Clustering Score by Model and PCA Count (Aggregated Over Scalers)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load selected results for plotting, just load the jsons and add the lists together like below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with open(\"k_means_results.json\", \"r\") as json_file:\n",
    "    km_results = json.load(json_file)\n",
    "\n",
    "with open(\"mean_shitf_results.json\", \"r\") as json_file:\n",
    "    ms_results = json.load(json_file)\n",
    "\n",
    "with open(\"affinity_propagation_results.json\", \"r\") as json_file:\n",
    "    ap_results = json.load(json_file)\n",
    "\n",
    "with open(\"birch_results.json\", \"r\") as json_file:\n",
    "    b_results = json.load(json_file)\n",
    "\n",
    "full_results = km_results + ms_results + b_results + ap_results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_scaling_impact_on_homogeneity_bar(full_results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_pca_impact_on_accuracy_best_result(full_results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clustering Final Evaluation / Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method for evaluation. Important to note that we are splitting with the same seed, so we train on the same train and test on the same test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import davies_bouldin_score, homogeneity_score\n",
    "\n",
    "def evaluate_clustering(estimator, X, y, pca_count=None, scaler=None):\n",
    "\n",
    "    if pca_count is not None:\n",
    "        pca = PCA(n_components=pca_count)\n",
    "        X = pd.DataFrame(pca.fit_transform(X), index=X.index)\n",
    "\n",
    "    if scaler is not None:\n",
    "        numerical_columns = X.select_dtypes(include=['number']).columns.tolist()\n",
    "        columns_to_scale = [col for col in numerical_columns if X[col].nunique() > 2]\n",
    "        X = scale_selected_columns(X, columns_to_scale, scaler)\n",
    "\n",
    "    # splitting with the same seed, so we train on the same train and test on the same test\n",
    "    X_train, X_test, _, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "    \n",
    "    estimator.fit(X_train)\n",
    "    y_pred = estimator.predict(X_test)\n",
    "\n",
    "    # 4. Compute metrics\n",
    "    db_score = davies_bouldin_score(X_test, y_pred)\n",
    "    homogeneity = homogeneity_score(y_test, y_pred)\n",
    "    custom_score = clustering_accuracy(y_test, y_pred)\n",
    "\n",
    "    print(f\"Davies-Bouldin Score: {db_score}\")\n",
    "    print(f\"Homogeneity Score: {homogeneity}\")\n",
    "    print(f\"Custom Scoring Function Output: {custom_score}\")\n",
    "\n",
    "    return estimator"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. KMeans Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km_best_params = {\n",
    "    \"n_clusters\": 50,\n",
    "    \"init\": \"random\",\n",
    "    \"max_iter\": 500,\n",
    "    \"tol\": 0.01,\n",
    "    \"algorithm\": \"lloyd\"\n",
    "}\n",
    "model = KMeans(**km_best_params)\n",
    "\n",
    "fitted_km = evaluate_clustering(model, X, y, pca_count=7, scaler=MinMaxScaler())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Affinty Propagation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "ap_params = {\n",
    "    \"damping\": 0.5,\n",
    "    \"preference\": 0\n",
    "}\n",
    "ap = AffinityPropagation(**ap_params)\n",
    "\n",
    "fitted_ap = evaluate_clustering(\n",
    "    estimator=ap, \n",
    "    X=X, \n",
    "    y=y, \n",
    "    pca_count=10, \n",
    "    scaler=MinMaxScaler()\n",
    ")\n",
    "\n",
    "labels_ap =fitted_ap.labels_\n",
    "n_clusters_ap = len(np.unique(labels_ap))\n",
    "print(n_clusters_ap)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. MeanShift Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "ms_params = {\n",
    "    \"bandwidth\": 1\n",
    "}\n",
    "\n",
    "ms = MeanShift(**ms_params)\n",
    "\n",
    "# Use scaler=None to skip scaling\n",
    "fitted_ms = evaluate_clustering(\n",
    "    estimator=ms, \n",
    "    X=X, \n",
    "    y=y, \n",
    "    pca_count=10, \n",
    "    scaler=None\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4. Birch Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.cluster import Birch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "birch_params = {\n",
    "    \"n_clusters\": 50,\n",
    "    \"threshold\": 0.5,\n",
    "    \"branching_factor\": 20\n",
    "}\n",
    "\n",
    "birch_model = Birch(**birch_params)\n",
    "\n",
    "fitted_birch = evaluate_clustering(\n",
    "    estimator=birch_model, \n",
    "    X=X, \n",
    "    y=y, \n",
    "    pca_count=10, \n",
    "    scaler=StandardScaler()\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
